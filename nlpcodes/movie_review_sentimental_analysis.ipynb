{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b71c86",
   "metadata": {},
   "source": [
    "## Movie_Review_Sentimental_Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0979ee3",
   "metadata": {},
   "source": [
    "### <b><u> In this notebook we will do movie review sentimental analysis using simple ANN model. For this we use *Tensorflow, nltk* libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708e3a01",
   "metadata": {},
   "source": [
    "<b><u>Importing Libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80cf129c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 01:02:10.368000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-04 01:02:10.459913: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-04 01:02:10.463426: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-04 01:02:10.463437: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-04 01:02:10.481775: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-05-04 01:02:10.899908: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-04 01:02:10.899961: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-04 01:02:10.899965: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# To remove punctuation\n",
    "import string\n",
    "\n",
    "# Array operation\n",
    "import numpy as np\n",
    "\n",
    "#For Dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# Regex Operation\n",
    "import re\n",
    "\n",
    "import os\n",
    "\n",
    "# Stopword remove\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Deep Neural Network\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524fc5e5",
   "metadata": {},
   "source": [
    "#### <u> function to read contents from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af91adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    \n",
    "    # open file read only\n",
    "    file = open(filename,'r')\n",
    "    \n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    \n",
    "    # close file\n",
    "    file.close()\n",
    "    \n",
    "    # return text data\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c63c202",
   "metadata": {},
   "source": [
    "#### <u> function to clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d035f82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    \n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    \n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile(\"[%s]\" % re.escape(string.punctuation))\n",
    "    \n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub( \"\", w) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word)>1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f4056",
   "metadata": {},
   "source": [
    "#### <u> function to convert document to lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fed268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load docs, clean and return line of tokens\n",
    "def doc_to_line(filename,vocab):\n",
    "    \n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    \n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    \n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8633e0ad",
   "metadata": {},
   "source": [
    "#### <u> file vocab file loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1c5845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_filename = '../dataset/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de71bcb3",
   "metadata": {},
   "source": [
    "#### <u> function to read all files from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11f3e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs from dictionary\n",
    "\n",
    "def process_docs(directory,vocab):\n",
    "    \n",
    "    lines = list()\n",
    "    \n",
    "    #walk through all files and folders\n",
    "    for filename in os.listdir(directory):\n",
    "        \n",
    "        #create the full path\n",
    "        path = directory + \"/\" + filename\n",
    "        \n",
    "        #load and clean data\n",
    "        line = doc_to_line(path,vocab)\n",
    "        \n",
    "        #add to list\n",
    "        lines.append(line)\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76693764",
   "metadata": {},
   "source": [
    "#### <u> function to load and clean entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8b1ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(vocab):\n",
    "    \n",
    "    # load documents\n",
    "    neg = process_docs('../dataset/movie_review/neg',vocab)\n",
    "    pos = process_docs('../dataset/movie_review/pos',vocab)\n",
    "    \n",
    "    docs = neg + pos\n",
    "    \n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    \n",
    "    return docs,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2dbf4b",
   "metadata": {},
   "source": [
    "#### <u> function to fit a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "378a0e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    \n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ab4787",
   "metadata": {},
   "source": [
    "#### <u> function to define and create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa481ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(n_words):\n",
    "    \n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Dense Layer 1\n",
    "    model.add(Dense(50,\n",
    "                   input_shape = (n_words,),\n",
    "                   activation = 'relu'))\n",
    "    \n",
    "    # Dense Layer 2 (output layer)\n",
    "    model.add(Dense(1,\n",
    "                    activation = 'sigmoid'))\n",
    "    \n",
    "    # compilation\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                 optimizer = 'adam',\n",
    "                 metrics = ['accuracy'])\n",
    "    \n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    \n",
    "    # plot model\n",
    "    plot_model(model,\n",
    "              to_file = 'model.png',\n",
    "              show_shapes = True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c5e39",
   "metadata": {},
   "source": [
    "#### <u> load all reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "680756d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "train_docs , ytrain = load_clean_dataset(vocab)\n",
    "\n",
    "# test dataset\n",
    "test_docs , ytest = load_clean_dataset(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5911cba",
   "metadata": {},
   "source": [
    "#### <u> create the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e90a5771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is object\n",
    "tokenizer = create_tokenizer(train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f124e3aa",
   "metadata": {},
   "source": [
    "#### <u> encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d18ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text doc to binary matrix\n",
    "#(i.e. if word present 1 else 0)\n",
    "\n",
    "X_train = tokenizer.texts_to_matrix(train_docs, mode = 'binary')\n",
    "\n",
    "X_test = tokenizer.texts_to_matrix(test_docs, mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf58d9",
   "metadata": {},
   "source": [
    "#### <u> Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9552f23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 50)                1288450   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 01:03:16.581992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-04 01:03:16.582524: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-04 01:03:16.582988: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-04 01:03:16.583375: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-04 01:03:16.583668: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-05-04 01:03:16.583762: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-05-04 01:03:16.584127: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-04 01:03:16.584415: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-04 01:03:16.585917: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-05-04 01:03:16.587025: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "n_words = X_train.shape[1]\n",
    "model = define_model(n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad2925d",
   "metadata": {},
   "source": [
    "#### <u> fit the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07e2db8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "201/201 [==============================] - 2s 7ms/step - loss: 0.3940 - accuracy: 0.8284 - val_loss: 0.0365 - val_accuracy: 0.9940\n",
      "Epoch 2/10\n",
      "201/201 [==============================] - 1s 6ms/step - loss: 0.0291 - accuracy: 0.9945 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "201/201 [==============================] - 1s 6ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "201/201 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "201/201 [==============================] - 1s 6ms/step - loss: 8.5307e-04 - accuracy: 1.0000 - val_loss: 5.3243e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "201/201 [==============================] - 1s 6ms/step - loss: 4.6645e-04 - accuracy: 1.0000 - val_loss: 3.1795e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "201/201 [==============================] - 1s 6ms/step - loss: 2.8519e-04 - accuracy: 1.0000 - val_loss: 2.0505e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "201/201 [==============================] - 1s 6ms/step - loss: 1.8820e-04 - accuracy: 1.0000 - val_loss: 1.4183e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "201/201 [==============================] - 1s 6ms/step - loss: 1.3132e-04 - accuracy: 1.0000 - val_loss: 1.0256e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "201/201 [==============================] - 1s 6ms/step - loss: 9.6381e-05 - accuracy: 1.0000 - val_loss: 7.7928e-05 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f470f52ada0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,np.array(ytrain),\n",
    "          validation_data=[X_test,np.array(ytest)],\n",
    "         epochs=10,\n",
    "         batch_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d1cd30",
   "metadata": {},
   "source": [
    "#### <u> classify the review as negative or positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7ab163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review): # , vocab , tokenizer , model\n",
    "    \n",
    "    # clean\n",
    "    tokens = clean_doc(review)\n",
    "    \n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    \n",
    "    # convert to line\n",
    "    line = ' '.join(tokens)\n",
    "    \n",
    "    # encode\n",
    "    encoded = tokenizer.texts_to_matrix([line], mode = 'binary')\n",
    "    \n",
    "    # predict sentiments\n",
    "    yhat = model.predict(encoded,verbose = 0)\n",
    "    \n",
    "    # retrieve predicted percentege and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    \n",
    "    if round(percent_pos)==0:\n",
    "        \n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    \n",
    "    return percent_pos, 'POSITIVE'            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "558e5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'best movie ever!! it was great. i recommend it.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84521cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: best movie ever!! it was great. i recommend it.\n",
      "Sentiment: POSITIVE (61.4%)\n"
     ]
    }
   ],
   "source": [
    "percent , sentiment = predict_sentiment(text)\n",
    "\n",
    "\n",
    "print(f\"Review: {text}\\nSentiment: {sentiment} ({round(percent*100,2)}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
