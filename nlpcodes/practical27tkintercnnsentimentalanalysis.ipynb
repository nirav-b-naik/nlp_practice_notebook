{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524fc5e5",
   "metadata": {},
   "source": [
    "#### <u> Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80cf129c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-24 17:14:04.282346: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-24 17:14:04.405197: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-24 17:14:04.410304: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-24 17:14:04.410320: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-24 17:14:04.438104: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-24 17:14:05.056740: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-24 17:14:05.056801: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-24 17:14:05.056809: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPool1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55910d8e",
   "metadata": {},
   "source": [
    "#### <u> function to read contents from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af91adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    \n",
    "    # open file read only\n",
    "    file = open(filename,'r')\n",
    "    \n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    \n",
    "    # close file\n",
    "    file.close()\n",
    "    \n",
    "    # return text data\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb74b41",
   "metadata": {},
   "source": [
    "#### <u> file vocab file loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9879f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_filename = '/home/dai/Desktop/dai2022/modulewise/naturallanguageprocessing/datasets/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c63c202",
   "metadata": {},
   "source": [
    "#### <u> function to clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d035f82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc,vocab):\n",
    "    \n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    \n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile(\"[%s]\" % re.escape(string.punctuation))\n",
    "    \n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub( \"\", w) for w in tokens]\n",
    "    \n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    \n",
    "    tokens = ' '.join(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f4056",
   "metadata": {},
   "source": [
    "#### <u> load all docs in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0d1d7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory, vocab, is_train):\n",
    "    \n",
    "    documents = list()\n",
    "    \n",
    "    # walk through all files in the folder\n",
    "    for filename in os.listdir(directory):\n",
    "        \n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        \n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        \n",
    "        # create the full ppath of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        \n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        \n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc,vocab)\n",
    "        \n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "        \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc420c",
   "metadata": {},
   "source": [
    "#### <u> function to load and clean entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ea6d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(vocab,is_train):\n",
    "    \n",
    "    # load documents\n",
    "    neg = process_docs('/home/dai/Desktop/dai2022/modulewise/naturallanguageprocessing/datasets/review_polarity/txt_sentoken/neg',vocab,is_train)\n",
    "    pos = process_docs('/home/dai/Desktop/dai2022/modulewise/naturallanguageprocessing/datasets/review_polarity/txt_sentoken/pos',vocab,is_train)\n",
    "    \n",
    "    docs = neg + pos\n",
    "    \n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    \n",
    "    return docs,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65424bc",
   "metadata": {},
   "source": [
    "#### <u> function to fit a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f11b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tokenizer is object\n",
    "def create_tokenizer(lines):\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    \n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b06df9",
   "metadata": {},
   "source": [
    "#### <u> function for integer encode and pad documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "797bf34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    \n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    \n",
    "    # pad sequences\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding = 'post')\n",
    "    \n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d557d8a",
   "metadata": {},
   "source": [
    "#### <u> function to define and create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e59d5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(vocab_size,max_length):\n",
    "    \n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding layer\n",
    "    model.add(Embedding(vocab_size,100,\n",
    "                        input_length = max_length))\n",
    "    \n",
    "    # Convolution Layers\n",
    "    model.add(Conv1D(filters=32,\n",
    "                     kernel_size=8,\n",
    "                     activation='relu'))\n",
    "    \n",
    "    # Pool Layers\n",
    "    model.add(MaxPool1D(pool_size=2))\n",
    "    \n",
    "    # Flattern Layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Dense Layer 1\n",
    "    model.add(Dense(50,\n",
    "                   input_shape = (vocab_size,),\n",
    "                   activation = 'relu'))\n",
    "    \n",
    "    # Dense Layer 2 (output layer)\n",
    "    model.add(Dense(1,\n",
    "                    activation = 'sigmoid'))\n",
    "    \n",
    "    # compilation\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                 optimizer = 'adam',\n",
    "                 metrics = ['accuracy'])\n",
    "    \n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    \n",
    "    # plot model\n",
    "    plot_model(model,\n",
    "              to_file = 'model_cnn.png',\n",
    "              show_shapes = True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166ac68c",
   "metadata": {},
   "source": [
    "#### <u> load all reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e147d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "train_docs , ytrain = load_clean_dataset(vocab,True)\n",
    "\n",
    "# test dataset\n",
    "test_docs , ytest = load_clean_dataset(vocab,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195fcea5",
   "metadata": {},
   "source": [
    "#### <u> create the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96706d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is object\n",
    "tokenizer = create_tokenizer(train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2a949a",
   "metadata": {},
   "source": [
    "#### <u> define vocabulaty size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3177b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 25768\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(f\"vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae12f43b",
   "metadata": {},
   "source": [
    "#### <u> calculate maximum sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f64a174b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Length = 1317\n"
     ]
    }
   ],
   "source": [
    "max_l_train = max([len(s.split()) for s in train_docs])\n",
    "\n",
    "max_l_test = max([len(s.split()) for s in test_docs])\n",
    "\n",
    "max_length = max(max_l_train,max_l_test)\n",
    "\n",
    "print(f\"Maximum Length = {max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc80ea",
   "metadata": {},
   "source": [
    "#### <u> encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92596561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text doc to binary matrix\n",
    "#(i.e. if word present 1 else 0)\n",
    "\n",
    "Xtrain = encode_docs(tokenizer,max_length,train_docs)\n",
    "\n",
    "Xtest = encode_docs(tokenizer,max_length,test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24e81609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1810, 1317), 1810)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, len(ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7decb59a",
   "metadata": {},
   "source": [
    "#### <u> Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01fca6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-24 17:14:40.243668: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-12-24 17:14:40.243766: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dai-Precision-7820-Tower): /proc/driver/nvidia/version does not exist\n",
      "2022-12-24 17:14:40.254703: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1317, 100)         2576800   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 1310, 32)          25632     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 655, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 20960)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                1048050   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,650,533\n",
      "Trainable params: 3,650,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_model(vocab_size,max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced44aca",
   "metadata": {},
   "source": [
    "#### <u> fit the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e531ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "181/181 [==============================] - 6s 25ms/step - loss: 0.6855 - accuracy: 0.5481 - val_loss: 0.6552 - val_accuracy: 0.6250\n",
      "Epoch 2/10\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 0.3117 - accuracy: 0.8646 - val_loss: 0.3696 - val_accuracy: 0.8800\n",
      "Epoch 3/10\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 0.0132 - accuracy: 0.9983 - val_loss: 0.4075 - val_accuracy: 0.8600\n",
      "Epoch 4/10\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4491 - val_accuracy: 0.8650\n",
      "Epoch 5/10\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 5.6481e-04 - accuracy: 1.0000 - val_loss: 0.4647 - val_accuracy: 0.8700\n",
      "Epoch 6/10\n",
      "181/181 [==============================] - 4s 24ms/step - loss: 2.9681e-04 - accuracy: 1.0000 - val_loss: 0.5220 - val_accuracy: 0.8700\n",
      "Epoch 7/10\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 1.6166e-04 - accuracy: 1.0000 - val_loss: 0.5566 - val_accuracy: 0.8700\n",
      "Epoch 8/10\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 9.2873e-05 - accuracy: 1.0000 - val_loss: 0.5872 - val_accuracy: 0.8700\n",
      "Epoch 9/10\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 5.7923e-05 - accuracy: 1.0000 - val_loss: 0.5992 - val_accuracy: 0.8700\n",
      "Epoch 10/10\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 3.9597e-05 - accuracy: 1.0000 - val_loss: 0.6161 - val_accuracy: 0.8700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ac27c3610>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtrain,np.array(ytrain),\n",
    "          validation_data= [Xtest,np.array(ytest)],\n",
    "          epochs=10,\n",
    "          batch_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17afdb13",
   "metadata": {},
   "source": [
    "#### <u> evaluate on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e826c0da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 18ms/step - loss: 3.0148e-05 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.0148083169478923e-05, 1.0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(Xtrain,np.array(ytrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace0af3c",
   "metadata": {},
   "source": [
    "#### <u> evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "572c334e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 17ms/step - loss: 0.6161 - accuracy: 0.8700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6160975098609924, 0.8700000047683716]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(Xtest,np.array(ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff48dee",
   "metadata": {},
   "source": [
    "#### <u> classify the review as negative or positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99092e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review): # , vocab , tokenizer , model\n",
    "    \n",
    "    # clean review\n",
    "    line = clean_doc(review, vocab)\n",
    "    \n",
    "    #eoncode and padd review\n",
    "    padded = encode_docs(tokenizer,max_length,[line])\n",
    "    \n",
    "    # predict sentiments\n",
    "    yhat = model.predict(padded,verbose = 0)\n",
    "    \n",
    "    # retrieve predicted percentege and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    \n",
    "    if round(percent_pos)==0:\n",
    "        \n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    \n",
    "    return percent_pos, 'POSITIVE'         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f199e98",
   "metadata": {},
   "source": [
    "#### <u> positive review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10f8542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Everyone will enjoy this film. I love it, recommended!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f585755f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: Everyone will enjoy this film. I love it, recommended!\n",
      "Sentiment: NEGATIVE (51.12%)\n"
     ]
    }
   ],
   "source": [
    "percent , sentiment = predict_sentiment(text)\n",
    "\n",
    "\n",
    "print(f\"Review: {text}\\nSentiment: {sentiment} ({round(percent*100,2)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847daa86",
   "metadata": {},
   "source": [
    "#### <u> negative review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d68a29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'this is bad movie. Do not watch it. It sucks.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eab512e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: this is bad movie. Do not watch it. It sucks.\n",
      "Sentiment: NEGATIVE (57.59%)\n"
     ]
    }
   ],
   "source": [
    "percent , sentiment = predict_sentiment(text)\n",
    "\n",
    "\n",
    "print(f\"Review: {text}\\nSentiment: {sentiment} ({round(percent*100,2)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4747f2",
   "metadata": {},
   "source": [
    "### <b><u>Save the object of classifier and vectorier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34c79ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98c016c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://bb907e3c-e785-4a38-976e-b4df3aacfc5b/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://bb907e3c-e785-4a38-976e-b4df3aacfc5b/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sentiment_model_27.model']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model,'sentiment_model_27.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b023262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentiment_tokenizer_27.model']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tokenizer,'sentiment_tokenizer_27.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f17382",
   "metadata": {},
   "source": [
    "### <b><u> Creating UI using tkinter for above model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6719e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f620026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('sentiment_model_27.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea2575cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = joblib.load('sentiment_tokenizer_27.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "798feb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = Tk()\n",
    "\n",
    "top.title('Sentiment Analysis')\n",
    "top.geometry(\"500x350\")\n",
    "\n",
    "def show():\n",
    "    \n",
    "    # converting text to str type\n",
    "    msg = str(text.get())\n",
    "    \n",
    "    # predict on model\n",
    "    percent , sentiment = predict_sentiment(msg)\n",
    "    \n",
    "    # printing prediction on UI\n",
    "    Label(text = f\"Review: {text}\\nSentiment: {sentiment} ({round(percent*100,2)}%)\").place(x=150,y=200)\n",
    "\n",
    "text = StringVar()  # intvar ,doublevar, stringvar\n",
    "\n",
    "l = Label(text = \"Enter Sentance: \").place(x=50,y=100)\n",
    "e = Entry(textvariable=text).place(x=200,y=100)\n",
    "b = Button(text = \"Submit\", command=show).place(x=180,y=150)\n",
    "\n",
    "top.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a1683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
